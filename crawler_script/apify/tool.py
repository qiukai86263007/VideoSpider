from langchain_community.utilities import ApifyWrapper
from langchain.docstore.document import Document
import ast


def craw_web(q: str):
    """
    爬取网页内容
    """
    apify = ApifyWrapper()
    loader = apify.call_actor(
        actor_id="apify/google-search-scraper",
        run_input={
            "queries": q,
        },
        dataset_mapping_function=lambda item: Document(
            page_content=str(item["organicResults"]) or "",
            metadata={"source": item["searchQuery"]['term'].split('site:')[1]}
        ),
        memory_mbytes=16384
    )
    result = loader.load()
    if len(result) == 0:
        return ''
    observation = result
    total_num = 0
    for re in result:
        top_level_domain = re.metadata['source']
        contents = ast.literal_eval(re.page_content)
        contents_num = len(contents)
        total_num += contents_num
        if contents_num > 0:
            print(f"✅ {top_level_domain} : {contents_num}")
            for content in contents:
                print(f'''[{content['title']}]({content['url']})''')
        else:
            print(print(f"❌ {top_level_domain}"))
    print(f"Total: {total_num}")
    return observation


def craw_web_vedio(q: str):
    """
    爬取网页内容
    """
    apify = ApifyWrapper()
    loader = apify.call_actor(
        actor_id="web.harvester/google-videos-scraper",
        run_input={
            "queries": q,
            "mobileResults": True,
        },
        dataset_mapping_function=lambda item: Document(
            page_content=str(item["organicResults"]) or "",
            metadata={"source": item["searchQuery"]['term'].split('site:')[1]}
        ),
        memory_mbytes=16384
    )
    result = loader.load()
    if len(result) == 0:
        return ''
    observation = result
    total_num = 0
    for re in result:
        top_level_domain = re.metadata['source']
        contents = ast.literal_eval(re.page_content)
        contents_num = len(contents)

        if contents_num > 0:
            true_contents_num = 0
            for content in contents:
                if 'url' in content and content['url'].startswith('http'):
                    true_contents_num += 1
                    print(f'''[{content['title']}]({content['url']})''')
            if true_contents_num > 0:
                total_num += true_contents_num
                print(f"✅ {top_level_domain} : {contents_num}")
        else:
            print(print(f"❌ {top_level_domain}"))
    print(f"Total: {total_num}")
    return observation


def craw_ytb_vedio(q: str, n: int):
    """
    爬取网页内容
    """
    apify = ApifyWrapper()
    loader = apify.call_actor(
        actor_id="streamers/youtube-scraper",
        run_input={
            "downloadSubtitles": False,
            "hasCC": False,
            "hasLocation": False,
            "hasSubtitles": False,
            "is360": False,
            "is3D": False,
            "is4K": False,
            "isBought": False,
            "isHD": False,
            "isHDR": False,
            "isLive": False,
            "isVR180": False,
            "maxResultStreams": 0,
            "maxResults": n,
            "maxResultsShorts": 0,
            "preferAutoGeneratedSubtitles": False,
            "saveSubsToKVS": False,
            "searchKeywords": q,
            "sortingOrder": "relevance"
        },
        dataset_mapping_function=lambda item: Document(
            page_content=str(item) or "",
            metadata={"url": item["url"], "title": item["title"], 'id': item['id'], 'channel': item['channelName'],
                      "text": item['text']}
        ),
        memory_mbytes=16384
    )
    result = loader.load()
    if len(result) == 0:
        return ''
    observation = []
    for re in result:
        observation.append(re.metadata)
        print(re.metadata)
    return observation
